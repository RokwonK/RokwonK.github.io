---
title: "[운영체제] 효율적으로 물리 메모리 관리하기"
categories: CS
tags:
  - OS
  - 메모리
---  

컴퓨터 내부 장치들은 연산장치와 기억장치로 나누어진다.(+ 주변장치) 연산장치에는 CPU와 GPU가 존재하며, 기억장치로는 RAM, SSD, HDD 등이 있다. 연산장치는 일꾼으로 주어진 명령을 처리하는 역할을 할 뿐이며, 그 자체로 최대 처리 속도가 정해져있다.  

그렇다면, 컴퓨터의 속도는 오직 연산장치의 수준으로 결정되는가? 아니라는 것을 우리는 이미 잘 알고 있다. 연산장치는 기억장치로부터 데이터를 읽어온 다음 데이터를 처리한다. 따라서, **기억장치로부터 얼마나 빠르게 데이터를 읽어올 수 있는가**는 속도에 영향을 미친다. 하지만 이 과정에서 메모리의 크기, 장치와의 거리, 프로그램의 크기 등의 한계가 존재한다. 이러한 문제를 처리하고 결과적으로 **기억장치와의 상호작용을 효율적으로 처리하는 것이 운영체제 메모리 관리의 목적이다.**{: .font-highlight}  

<br />  

### 메모리를 계층으로 나누어 관리하자
메모리 관리 방법을 알아보기 전 하드웨어적으로 나누어진 메모리 시스템에 대해서 알아보자. 컴퓨터 내부에는 CPU 내 레지스터부터 캐시메모리, 물리메모리, 디스크 등 여러가지 기억장치가 존재한다. 전부 데이터를 저장하는 기억장치일 뿐인데 왜 이렇게 종류가 다양할까? **그냥 하나의 기억장치만 사용하면 되는 것 아닐까?** 하나의 기억장치만을 사용하게 된다면 다음과 같은 문제가 생길 수 있다.  

1. **물리적 거리**
  - CPU와 디스크가 거리가 멀다. 디스크에서 데이터를 가져오는 시간이 길기 때문에 CPU가 대기하는 시간이 길어진다.
2. **비용문제**
  - CPU와 거리가 가까워지고 메모리 크기가 커질수록 비용은 커진다. 모든 데이터를 관리할 수 있는 크기의 기억장치가 CPU와 가장 가깝게 존재한다면 그 비용이 어마어마할 것이다.
3. **속도저하**
  - 기억장치에는 DMA라는 매커니즘을 통해 CPU 뿐만 아니라 여러가지 장치들이 접근할 수 있다. 하나의 기억장치만을 사용한다면 동시 접근에 대한 처리를 하면서 전체적인 속도가 저하될 수 있다.  


따라서, 기억장치들을 여러 종류로 나누고 계층구조로 만들어 상호작용한다. CPU와 가까울수록 빠른 처리속도와 작은 용량을 가진다.(용량이 커질수록 비싸니까)  

**계층구조를 이루는 이유는 처리속도의 차이로 인한 병목현상을 해결**{: .font-highlight}하기 위해서이다. 대부분의 프로그램들은 하디디스크(혹은 SSD)에 저장되어 있다. 하지만, CPU와 하드디스크의 거리가 너무 멀기 때문에 CPU의 처리속도를 따라가지 못한다. 해서 용량은 작지만 CPU와 가깝게 메모리를 둔다. 하지만, CPU와 가까운 메모리는 용량이 작기 때문에 CPU가 원하는 데이터를 가지고 있을 확률이 적다. 따라서 이를 계층구조 두어 필요한 메모리를 다음 계층의 메모리로 요청하고 받아올 수 있다.  

![memory_layer](https://github.com/the-pool/the-pool-api/assets/52196792/a72cf1c5-5a39-401d-8b7f-1db505d8216a){: .align-center style="width: 30%;"}  
메모리 계층 구조
{: .image-caption style="font-size: 14px;" }  

<br />  

### 물리 메모리로의 접근을 추상화시키자
일반적으로 프로세스들은 직접적으로 메모리 주소를 사용하지 않고 **논리주소를 이용**{: .font-highlight}한다. **프로세스들이 생성되고 할당받는 메모리들은 전부 물리 메모리번지가 아닌 논리주소번지**이다. 논리주소는 실제로 존재하지 않고 논리적으로만 존재하는 주소이다. 따라서, 이를 실제로 존재하는 물리 메모리 주소로 바꿔줄 필요가 있다.  

CPU는 프로세스들이 접근하려는 논리주소(소스 코드상 주소)를 받아 MMU(Memory Management Unit)를 통해서 물리주소로 변환하여 접근하게 된다. 이렇게 **논리주소를 물리주소로 매핑하는 과정을 Address Binding(주소 바인딩)**이라고 부른다.  

![mmu](https://github.com/the-pool/the-pool-api/assets/52196792/a695cedb-b1e1-47dc-8f88-91453519cba1){: .align-center style="width: 60%;"}  
Address Binding(주소 바인딩)
{: .image-caption style="font-size: 14px;" }  

바로 물리주소로 접근하지 않고 논리주소를 이용하게 복잡하게 만든 것일까? 논리주소를 이용하여 주소번지를 추상화함에 따라 다음과 같은 장점을 얻을 수 있다.

1. **프로세스의 독립성 보장**  
  - 각 프로세스들은 가상주소를 통해서 본인들만의 독립적인 공간을 사용할 수 있다. 프로세스간 주소공간의 충돌을 최소화 할 수 있다.  
2. **메모리 보호**  
  - 프로세스가 다른 프로세스의 주소 공간에 접근하려고 할때, 운영체제나 MMU가 보안 검사를 수행하여 불법적인 접근을 차단할 수 있다.  
3. **가상 메모리 사용가능**{: .font-highlight}  
  - **가상 메모리는 논리주소를 사용하는 가장 핵심적인 이유**이다. 가상 메모리는 실제 존재하는 메모리의 크기를 더 크게 보이도록 하는 방법이다. 직접적으로 물리 메모리에 접근을 한다면 이런 기술을 사용할 수 없을 것이다. 가상 메모리에 대해서는 이후 더 자세하게 알아보자.
4. **이식성**  
  - 프로세스가 물리 메모리에 의존하지 않고 작업을 수행할 수 있다. 따라서 다른 시스템이나 환경으로 이식하는데 유리하다.  

즉, 주소를 추상화함으로써 **민감한 메모리를 안정적이고 효율적으로 관리**할 수 있는 것이다.  

<br />  

### 메모리에는 여러개의 프로세스가 올라가있는데?
메모리에는 다수의 프로세스가 현재 사용되는 부분들이 적재된다. 또한, **프로세스들이 공간을 할당받고 해제되는 과정이 반복적으로 일어난다.** 이 과정 속에서 메모리 공간을 낭비없이 효율적으로 사용하기 위해서는 공간을 분배하는 방식은 매우 중요하다. 

이때 연속할당 기법이 사용된다. 연속할당 기법이란 **이미 할당되어 있는 프로세스와 인접한 공간에 적재하는 방식**이다. 어찌 생각해보면 당연한 것이다. 할당된 공간 옆 공간을 사용해야 오밀조밀하게 공간을 잘 사용할 수 있기 때문이다. 연속할당 기법에는 두 가지가 방식이 존재한다. 메모리를 프로세스 크기에 맞춰 나누는 가변 분할 방식과 메모리를 고정된 크기로 나누는 고정 분할 방식이 있다.  

**가변분할 방식**{: .font-highlight}  
메모리를 프로세스의 크기에 맞추어 나누는 방식이다. 이 방식은 하나의 프로세스가 사용하는 메모리 공간이 모두 뭉쳐있다.(연속적으로 메모리 공간을 가지고 있다.)  

하지만 다수의 프로세스가 공간을 할당받고 해제되는 과정이 반복되면서 기존에 프로세스가 차지하고 있던 **메모리 공간의 크기가 새로 들어오는 프로세스 크기보다 작아 사용되지 못하는 상황이 발생한다. 이를 외부 단편화**라고 부른다.  

이러한 낭비를 줄이기 위해 조각 모음과 같이 사용중인 메모리들을 Compaction(압축)시키는 알고리즘이 동작하는데 이는 관리가 매우복잡하다. 따라서, 이 방식을 잘 사용하지 않는다.  

<br />  

**고정 분할 방식**{: .font-highlight}  
메모리를 고정된 크기로 나누고 들어오는 프로세스를 이 크기에 맞추는 분할하여 할당한다. 하나의 프로세스가 연속적인 메모리 공간을 가지지 않는다.  

이때 **프로세스를 고정된 크기로 나누고 남는 부분도 할당하면서 나누어진 메모리 공간 내부가 남는 경우가 발생한다. 이를 내부 단편화** 라고 부른다.  

메모리 낭비가 존재하지만 가변분할 방식에 비해 훨씬 효율적이라 현대에는 이 방식을 많이 사용한다.  